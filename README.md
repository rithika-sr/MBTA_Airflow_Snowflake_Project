\# ğŸš† MBTA Airflowâ€“Snowflake Data Pipeline



\### \*Automating Real-Time MBTA Transit Data with Airflow, Docker \& Snowflake\*



!\[Airflow + Snowflake](https://github.com/github/explore/raw/main/topics/airflow/airflow.png)



---



\## ğŸ“– Project Overview



This project demonstrates an \*\*end-to-end automated data pipeline\*\* that fetches \*\*real-time MBTA (Massachusetts Bay Transportation Authority)\*\* predictions using the official API and loads them into a \*\*Snowflake data warehouse\*\* every 10 minutes using \*\*Apache Airflow\*\* orchestrated inside Docker.



Itâ€™s designed as a \*\*resume-ready data engineering project\*\* â€” showcasing ETL automation, API ingestion, cloud data warehousing, and containerized orchestration.



---



\## ğŸ§© Tech Stack



| Category        | Tools \& Technologies              |

| --------------- | --------------------------------- |

| Orchestration   | ğŸª¶ Apache Airflow (Dockerized)    |

| Cloud Warehouse | â„ï¸ Snowflake                      |

| Programming     | ğŸ Python 3 (Pandas, Requests)    |

| Infrastructure  | ğŸ³ Docker Desktop                 |

| Scheduling      | â° Cron (Airflow DAG every 10 min) |

| Data Source     | ğŸš‡ MBTA Live Predictions API      |



---



\## âš™ï¸ Architecture



```mermaid

flowchart LR

&nbsp;   A\[ğŸš‡ MBTA API] --> B\[ğŸ Python Extraction]

&nbsp;   B --> C\[ğŸ§¹ Transform (Clean \& Handle NaN)]

&nbsp;   C --> D\[â„ï¸ Load into Snowflake]

&nbsp;   D --> E\[ğŸ“Š Query / Dashboard in Snowflake]

&nbsp;   F\[â° Airflow Scheduler] --> B

```



---



\## ğŸ“‚ Repository Structure



```

MBTA\_Airflow\_Project/

â”‚

â”œâ”€â”€ dags/

â”‚   â””â”€â”€ mbta\_to\_snowflake\_dag.py      # Main Airflow DAG

â”‚

â”œâ”€â”€ docker-compose.yaml               # Airflow Docker environment

â”œâ”€â”€ .gitignore                        # Ignore cache/logs/credentials

â””â”€â”€ README.md                         # Project documentation

```



---



\## ğŸš€ How It Works



1\. \*\*Fetch MBTA Data\*\*



&nbsp;  \* The DAG calls the MBTA API endpoint `/predictions`

&nbsp;  \* Extracts route, direction, status, arrival/departure times

&nbsp;  \* Saves results locally â†’ `/tmp/mbta\_data.csv`



2\. \*\*Load into Snowflake\*\*



&nbsp;  \* Reads the CSV into Pandas

&nbsp;  \* Creates table `MBTA\_LIVE\_PREDICTIONS` (if not exists)

&nbsp;  \* Inserts records via Snowflake Connector

&nbsp;  \* Handles `NaN` â†’ `NULL` conversion safely



3\. \*\*Schedule via Airflow\*\*



&nbsp;  \* DAG runs every 10 minutes (`\*/10 \* \* \* \*`)

&nbsp;  \* Tasks:



&nbsp;    \* ğŸŸ© `fetch\_mbta\_data` â†’ ğŸŸ© `load\_to\_snowflake`



4\. \*\*Monitor in Airflow UI\*\*



&nbsp;  \* Web UI â†’ \[http://localhost:8080](http://localhost:8080)

&nbsp;  \* View DAG status (âœ… success / âŒ failure / retry)



---



\## ğŸ§  Snowflake Table Schema



| Column         | Type      | Description              |

| -------------- | --------- | ------------------------ |

| ROUTE          | STRING    | MBTA line (e.g. Green-B) |

| DIRECTION      | INT       | Direction ID             |

| STATUS         | STRING    | Train status             |

| ARRIVAL\_TIME   | STRING    | Arrival timestamp        |

| DEPARTURE\_TIME | STRING    | Departure timestamp      |

| STOP\_SEQ       | INT       | Stop sequence            |

| LOAD\_TIMESTAMP | TIMESTAMP | Time of pipeline load    |



---



\## ğŸ› ï¸ Setup \& Run



\### 1ï¸âƒ£ Clone the Repository



```bash

git clone https://github.com/<your-username>/MBTA\_Airflow\_Project.git

cd MBTA\_Airflow\_Project

```



\### 2ï¸âƒ£ Start Airflow with Docker



```bash

docker compose up -d

```



Wait until containers (`webserver`, `scheduler`, `worker`, `postgres`, `redis`) are running.



\### 3ï¸âƒ£ Open Airflow UI



\[http://localhost:8080](http://localhost:8080)

Enable the DAG â†’ `mbta\_to\_snowflake\_dag`



\### 4ï¸âƒ£ Verify in Snowflake



```sql

SELECT \* 

FROM SNOWFLAKE\_LEARNING\_DB.PUBLIC.MBTA\_LIVE\_PREDICTIONS

ORDER BY LOAD\_TIMESTAMP DESC

LIMIT 20;

```



---



\## ğŸ§¾ Example Run Output



\*\*Airflow Logs\*\*



```

âœ… 10 MBTA records fetched and saved to /tmp/mbta\_data.csv  

âœ… Data inserted into Snowflake successfully!  

```



\*\*Snowflake Query Result\*\*



| ROUTE   | STATUS   | LOAD\_TIMESTAMP          |

| ------- | -------- | ----------------------- |

| Green-B | Arriving | 2025-10-07 18:00:00 UTC |

| Green-B | Departed | 2025-10-07 17:50:00 UTC |



---



\## ğŸ”’ Security \& Best Practices



\* âœ… Store credentials securely in Airflow Connections (not in code)

\* âœ… Use `.gitignore` to exclude logs, .env files, and CSVs

\* âœ… Parameterize account details for portability





---



\## ğŸŒŸ Key Highlights



\* ğŸ§  Real-time data integration pipeline

\* âš™ï¸ Automated Airflow scheduling inside Docker

\* â„ï¸ Snowflake data warehouse integration

\* ğŸ§¹ Robust error handling \& NaN management

\* ğŸ§¾ Production-style architecture for portfolio







